kube-prometheus-stack:
  # Configure the Prometheus server
  server:
    # The URL to scrape for metrics
    scrape_configs:
      - job_name: prometheus
        scrape_interval: 15s
        scrape_timeout: 10s
        metrics_path: /metrics
        scheme: http
        static_configs:
          - targets:
            - 338f-2a02-908-1d40-2380-7726-d37d-7758-4cf7.ngrok.io


  additionalPrometheusRulesMap:
    rule-name:
      groups:
        - name: my_group
          rules:
            - alert: alert pipeline check
              expr: vector(1)
              for: 1m
              labels:
                severity: warning
              annotations:
                summary: Alert pipeline works
        - name: "alert_rules"
          rules:
            - alert: RequestsDropped
              expr: requests_handled{job="my_microservice"} < 10
              for: 1m
              labels:
                severity: critical
              annotations:
                description: "The number of requests handled by the API has dropped below 10"
                summary: "Low request rate for {{ $labels.instance }}"
  prometheusSpec:
     additionalScrapeConfigs:
       - job_name: my_microservice
         scrape_interval: 5s
         scrape_timeout: 5s
         metrics_path: "/metrics"
         scheme: "http"
         static_configs:
           - targets:
               - "localhost:8000"


  # Configure the Alertmanager component
  alertmanager:
    enabled: true
    config:
      global:
        resolve_timeout: 5m
      route:
        group_by: [ 'job', 'alertname', 'priority' ]
        group_wait: 10s
        group_interval: 1m
        receiver: 'prometheus-msteams'
        routes:
          - match:
              alertname: Watchdog
            receiver: "null"
      receivers:
        - name: "null"
        - name: 'prometheus-msteams'
          webhook_configs:
            - url: "http://prometheus-msteams:2000/team_connector"
              send_resolved: true
  # Enable the node_exporter component to collect metrics from the Kubernetes nodes
  node_exporter:
    enabled: true

  # Enable the kube-state-metrics component to collect metrics about the state of the Kubernetes cluster
  kube_state_metrics:
    enabled: true

  # Enable the cAdvisor component to collect metrics about the containers running in the cluster
  cAdvisor:
    enabled: true

  grafana:
    grafana.ini:
      feature_toggles:
        enable: tempoSearch      # To enable search option for tempo traces on Grafana
    persistence:
      type: statefulset          # To deploy Grafana as a StatefulSet
      enabled: true
      size: 5Gi                  # Size of volume used by Grafana for storing dashboards created from the Grafana UI
    datasources: # Omit this if no additional datasources are required
      datasources.yaml:
        apiVersion: 1
        datasources:
          - name: Loki             # Adding Loki as datasource
            type: loki
            uid: Loki
            access: proxy
            editable: false
            url: http://loki-gateway.monitoring:80     # DNS of loki-gateway
            jsonData: # To setup integration between Loki and Tempo
              derivedFields:
                - datasourceUid: Tempo
                  matcherRegex: "(?:traceID|traceId)(?:=|\\s)(\\w+)"
                  name: TraceID
                  url: "$${__value.raw}"
          - name: Tempo                         # Adding Tempo datasource
            type: tempo
            uid: Tempo
            access: proxy
            editable: false
            url: http://tempo-gateway.monitoring:80
    sidecar:
      dashboards:
        enabled: true                        # To enable dashboards as configMaps automatically picked up by Grafana
        label: grafana_dashboard             # Label the configMaps should have in order to be added
        folder: /tmp/dashboards
        searchNamespace: ALL                 # To look for dashboard configMaps in all namespaces
